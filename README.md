# llm_cache
A simple design of a cache for faster LLM responses at inference time
